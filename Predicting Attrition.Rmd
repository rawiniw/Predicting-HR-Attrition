---
title: "Predicting HR Attrition"
output:
  md_document:
  variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message = FALSE}    
library(dplyr)
library(tidyr)
library(ggplot2)
library(Amelia)
```

```{r}
#read in merged data, created by conmbining several spreadsheets
Master_Data <- read.csv("master.csv")
```

```{r}
##CLEAN DATA

#Missing Data Map
missmap(Master_Data,y.at=c(1),y.labels = c(''),col=c('yellow','black'))

#Very few rows with missing data - omit
Master_Data <- na.omit(Master_Data)

#Remove columns we dont need in our analysis
Master_Data$StandardHours <- NULL
Master_Data$Over18 <- NULL
Master_Data$EmployeeCount <- NULL
Master_Data$EmployeeID <- NULL
```

# Exploratory Data Analysis
```{r}
ggplot(Master_Data,aes(Age)) + 
    geom_histogram(aes(fill=Attrition),color='black',binwidth=10) 
```

```{r}
ggplot(Master_Data,aes(JobRole)) +
  geom_bar(aes(fill = Attrition),color = 'black') +
  theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
ggplot(Master_Data,aes(Avg_Daily_Hours)) + 
    geom_histogram(aes(fill=Attrition),color='black',binwidth=1) 
```

# Logistic Regression Prediction:

```{r}
##SPLIT DATA
library(caTools)
set.seed(1)
sample <- sample.split(Master_Data$Attrition,SplitRatio = 0.7)
#Training Data
train <- subset(Master_Data,sample==T)
#Testing Data
test <- subset(Master_Data,sample==F)
```

```{r}
##TRAIN THE LOGISTIC REGRESSION MODEL
logModel <- glm(Attrition ~., family = binomial('logit'),data = train)
summary(logModel)
```

```{r}
##PREDICTIONS
pred_probabilities <- predict(logModel,test,type = 'response')
pred_results <- ifelse(pred_probabilities>0.5,1,0)
#Convert Attrition column in test to 0s and 1s to compare to pred_results
test$AttritionClass <- ifelse(test$Attrition == 'Yes',1,0)
misClassError <- mean(pred_results != test$AttritionClass)
accuracy <- (1-misClassError)
cat("Misclassification error:", misClassError, "\n")
cat("Accuracy:", accuracy, "\n")
```

```{r}
#Confusion matrix for Logistic Regression Model
cm_log = table(test$AttritionClass, pred_results)
cm_log

cat(100 * cm_log[2, 2]/(cm_log[1, 2] + cm_log[2, 2]), "% of the people who predicted to leave the company actually left.\n")
cat(100 * cm_log[1, 1]/(cm_log[1, 1] + cm_log[2, 1]), "% of the people who predicted to stay with the company actually stayed.")
```


# LASSO Regression Prediction:

```{r}
#LASSO to determine variables to include
library(glmnet)
trainMatrix <- model.matrix(Attrition ~ ., data = train)
testMatrix <- model.matrix(Attrition ~ ., data = test)
grid <- 10 ^ seq(4, -2, length = 100)
lasso <- glmnet(trainMatrix, train$Attrition, alpha = 1, lambda = grid, thresh = 1e-12, family = "binomial")
lassoCV <- cv.glmnet(trainMatrix, train$Attrition, alpha = 1, lambda = grid, thresh = 1e-12, family = "binomial")
lassoLambdaMin <- lassoCV$lambda.min
predict(lasso, s = lassoLambdaMin, type = "coefficients")
```

```{r}
testMatrix = testMatrix[,-43]
```

```{r}
pred_probabilitiesLASSO <- predict(lasso, s = lassoLambdaMin, type = "response", newx = testMatrix)
pred_resultsLASSO <- ifelse(pred_probabilitiesLASSO>0.5,1,0)
#Convert Attrition column in test to 0s and 1s to compare to pred_results
#test$AttritionClass <- ifelse(test$Attrition == 'Yes',1,0)
misClassErrorLASSO <- mean(pred_resultsLASSO != test$AttritionClass)
accuracyLASSO <- (1-misClassErrorLASSO)
cat("Misclassification error:", misClassErrorLASSO, "\n")
cat("Accuracy:", accuracyLASSO, "\n")
```
```{r}
#Confusion matrix for LASSO Regression Model
cm_LASSO = table(test$AttritionClass, pred_resultsLASSO)
cm_LASSO

cat(100 * cm_LASSO[2, 2]/(cm_LASSO[1, 2] + cm_LASSO[2, 2]), "% of the people who predicted to leave the company actually left.\n")
cat(100 * cm_LASSO[1, 1]/(cm_LASSO[1, 1] + cm_LASSO[2, 1]), "% of the people who predicted to stay with the company actually stayed.")
```

Interestingly, LASSO Regression yielded the exact same misclassification error and accuracy as the Logistic Regression model. 
However, the confusion matrix elements between the two are different. The LASSO model does a much better job at predicting the people that would actually leave the company. It does so at the expense of lower prediction accuracy of those that would stay at the company.

```{r}
plot(lasso, xvar = "dev")
plot(lasso, xvar = "lambda")
plot(lasso, xvar = "norm")
```


# Random Forest prediction using significant variables identified by LASSO:

```{r}
library(randomForest)
set.seed(1)
#Drop the variables whose coefficients = 0 in LASSO
#Produces a simpler model without compromising accuracy.
Master_Data <- Master_Data[,-c(4,5,6,7,8,9,12,14,15,18,24,25,27,28)]
sample <- sample.split(Master_Data$Attrition,SplitRatio = 0.7)
#Training Data
train <- subset(Master_Data,sample==T)
#Testing Data
test <- subset(Master_Data,sample==F)
set.seed(1)
finrf = randomForest(Attrition~.,data=train,ntree=500)
finrfpred=predict(finrf,newdata=test)
cat("Misclassification error:", sum(abs(as.numeric(test$Attrition)-as.numeric(finrfpred)))/nrow(test))
finrfrmse = sqrt(sum((as.numeric(test$Attrition)-as.numeric(finrfpred))^2)/nrow(test))
varImpPlot(finrf)
```

Avg_Daily_Hours, Age, TotalWorkingYears, and JobRole are the most significant variables in predicting attrition.

```{r}
#Confusion matrix for Random Forest Model
cm_rf = table(test$Attrition, finrfpred)
cm_rf

cat(100 * cm_rf[2, 2]/(cm_rf[1, 2] + cm_rf[2, 2]), "% of the people who predicted to leave the company actually left.\n")
cat(100 * cm_rf[1, 1]/(cm_rf[1, 1] + cm_rf[2, 1]), "% of the people who predicted to stay with the company actually stayed.")
```

# Model Performance Evaluation
Comparing the three different models that were developed, the Random Forest model built off of the LASSO variables comes out on top. This model produced the highest prediction accuracy (.978), while minimizing the occurrence of obtaining a false positive or negative. In fact, the Random Forest model reduced the occurrence of a false negative by a factor of 10 from the
original Logistic (.39 vs .031). Assessing out of sample misclassification error, we see that LASSO and Logistic produce the same value of .15. However, if prompted to select the better model out of the two, we would suggest using LASSO. This model reduces complexity by zeroing out many of the variables in addition to cutting the occurrence of a false negative by half
(.39 to .18).